{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b138c5c-6a23-4adb-ab7f-b6c4da86467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.marketwatch.com/markets/us\n",
    "# https://www.marketwatch.com/investing?mod=top_nav look at key industries \n",
    "# https://www.marketwatch.com/investing/stock/nvda you can look at single stocks to predit as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a908249b-7656-46ca-8a2d-a87f82d4dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bloomberg scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from chromedriver_py import binary_path\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timezone, time as dt_time\n",
    "import json\n",
    "import threading\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from google.cloud import storage\n",
    "import uuid\n",
    "import traceback\n",
    "from google.cloud import storage\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import string\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./svc_acc_key.json\"\n",
    "MONGO_URI=os.environ[\"MONGO_URI\"]\n",
    "OPENAI_API_KEY=os.environ[\"OPENAI_API_KEY\"]\n",
    "STORAGE_BUCKET=os.environ[\"STORAGE_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb4d138-a350-4b9c-80f4-aaef4d0f5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "storage_client = storage.Client.from_service_account_json(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\n",
    "bucket = storage_client.get_bucket(STORAGE_BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205711ea-876c-48fd-92e8-43f960a9858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(MONGO_URI, server_api=ServerApi('1'))\n",
    "db = client.get_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba31e24-0396-47f8-b77b-3afd48c432bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db():\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dd5826-49c8-4259-917c-f7500d67064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stocks_list():\n",
    "    filename = \"stocks_list.txt\"\n",
    "    with open(filename, 'r') as file:\n",
    "        file_contents = file.read().split(\"\\n\")\n",
    "        return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb1f56d-9871-4120-bb3b-2bfd4b48c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_storage(articles, stock_sym, timestamp, run_id):\n",
    "        if not articles:\n",
    "            return \n",
    "\n",
    "        time_as_str_formatted = timestamp.strftime('%Y-%m-%d-%H-%M-%S').replace('-', '_')\n",
    "        directory = f\"scrapes/{run_id}/{stock_sym.lower()}/yahoo\"\n",
    "        \n",
    "        db = get_db()\n",
    "        scrapes_collection = db['scrapes']\n",
    "        scrapes = []\n",
    "\n",
    "        for article in articles:\n",
    "            sanitized_title = re.sub(r'[\\/:*?\"<>|]', '', article['title']).lower().translate(str.maketrans('', '', string.punctuation)).replace(\" \", \"_\")\n",
    "            key = f\"{directory}/{sanitized_title}.txt\"\n",
    "            new_blob = bucket.blob(key)\n",
    "\n",
    "            try: \n",
    "                new_blob.upload_from_string(json.dumps(article))\n",
    "            except Exception as e:\n",
    "                self.logger.info(\"[scraper]: failed to save article to cloud bucket\")\n",
    "                continue\n",
    "\n",
    "            app_env = os.environ.get('APP_ENV', 'LOCAL')\n",
    "            scrape = {\n",
    "                \"stock\": stock_sym.lower(),\n",
    "                \"scraped_at\": timestamp,\n",
    "                \"bucket_key\": key,\n",
    "                \"app_env\": app_env,\n",
    "                \"source\": \"yahoo\",\n",
    "                \"url\": article[\"link\"],\n",
    "                \"run_id\": run_id,\n",
    "            }\n",
    "\n",
    "            if article['published_at']:\n",
    "                parsed_time = datetime.strptime(article['published_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                scrape['published_at'] = parsed_time\n",
    "\n",
    "            scrapes.append(scrape)\n",
    "        scrapes_collection.insert_many(scrapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2cd70-2795-4927-859e-f9628c61e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_published_at(soup):\n",
    "    datetime_value = None\n",
    "    # Find the <time> tag with the specific class\n",
    "    time_tag = soup.find('time', class_='byline-attr-meta-time')\n",
    "    if time_tag:\n",
    "        datetime_value = time_tag['datetime']\n",
    "    else:\n",
    "        time_wrapper = soup.find('div', class_='caas-attr-time-style') \n",
    "        if time_wrapper:\n",
    "            time_tag = time_wrapper.find('time')\n",
    "            if time_tag:\n",
    "                datetime_value = time_tag['datetime']\n",
    "\n",
    "    return datetime_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faffc20-690b-4013-8705-dcba965eac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(soup, link):\n",
    "    article_text = []\n",
    "    article_content = soup.find('div', class_=(\"caas-body\", \"body yf-5ef8bf\"))\n",
    "    if not article_content:\n",
    "        print(f\"skipped link: {link}\")\n",
    "        return\n",
    "        \n",
    "    p_tags = article_content.find_all('p')\n",
    "    if not p_tags:\n",
    "        return\n",
    "        \n",
    "    for p_tag in p_tags:\n",
    "        ptext = p_tag.get_text().strip()\n",
    "        article_text.append(ptext)\n",
    "        \n",
    "    article_text_str = '\\n'.join(article_text)\n",
    "    return article_text_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf27b7-f2d7-4cca-95a4-39a93b74180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recent_news_for_sym(link, driver):\n",
    "    driver.get(link)\n",
    "    time.sleep(3)\n",
    "    # self.wait_for_article_body(driver)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    title = driver.title\n",
    "    published_at = get_published_at(soup)\n",
    "    if not published_at:\n",
    "        print(f\"[scraper] No published at found for {title}\")\n",
    "\n",
    "    article_text_str = get_article_content(soup, link)\n",
    "    if not article_text_str:\n",
    "        return\n",
    "        \n",
    "    res = {\n",
    "        \"content\": article_text_str, \n",
    "        \"title\": title, \n",
    "        \"link\": link,\n",
    "    }\n",
    "\n",
    "    if published_at:\n",
    "        res[\"published_at\"] = published_at\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81802cb-da07-42e3-94b3-5f5a1b7ed2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_close(soup):\n",
    "    prev_close_price = soup.find('fin-streamer', {\n",
    "        'class': 'yf-mrt107',\n",
    "        'data-field': 'regularMarketPreviousClose'\n",
    "    })\n",
    "\n",
    "    data_value = None\n",
    "    # Extract the data-value attribute\n",
    "    if prev_close_price:\n",
    "        data_value = prev_close_price.get('data-value')\n",
    "        # print(f\"prev close Data Value: {data_value}\")\n",
    "    \n",
    "    return data_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e5e7a-9d61-46c4-952f-21ec40998a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_market_price_for_stock(soup):\n",
    "    post_market_price = soup.find('fin-streamer', {\n",
    "        'class': 'price yf-1tejb6',\n",
    "        'data-field': 'postMarketPrice'\n",
    "    })\n",
    "    \n",
    "    data_value = None\n",
    "    # Extract the data-value attribute\n",
    "    if post_market_price:\n",
    "        data_value = post_market_price.get('data-value')\n",
    "        print(f\"post_market_price Data Value: {data_value}\")\n",
    "    return data_value\n",
    "\"\"\"\n",
    "do an OR bc sometimes you're running at nigth and sometimes at day\n",
    "<fin-streamer class=\"price yf-1tejb6\" data-symbol=\"XOM\" data-testid=\"qsp-pre-price\" data-field=\"preMarketPrice\" data-trend=\"none\" data-pricehint=\"2\" data-value=\"123.933\" active=\"\"><span class=\"d60f3b00 fc6ee16d\">123.94</span></fin-streamer>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a7332-f683-438f-8240-9dbebebd859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_for_stock(url, opts, svc):\n",
    "    try:\n",
    "        with webdriver.Chrome(service=svc, options=opts) as driver:\n",
    "            articles_for_stock = set()\n",
    "            driver.get(url)\n",
    "            print(\"Letting page load...\")\n",
    "            time.sleep(5)\n",
    "            # self.wait_for_stock_article_links(driver)\n",
    "           \n",
    "            main_page_source = driver.page_source\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            filtered_stories = soup.find('div', class_=lambda x: x and 'filtered-stories' in x)\n",
    "            if not filtered_stories:\n",
    "                print(f\"[scraper]: No filtered stories found for url {url}\")\n",
    "                raise ValueError(\"no stories found\")\n",
    "                \n",
    "            atags = filtered_stories.find_all(\"a\", class_=lambda x: x and 'subtle-link' in x)\n",
    "            if not atags:\n",
    "                print(f\"scraper] No atags found for url {url}\")\n",
    "                raise ValueError(\"no tags found\")\n",
    "                \n",
    "            for atag in atags:\n",
    "                link = atag.get('href')\n",
    "                if not link:\n",
    "                    continue\n",
    "            \n",
    "                articles_for_stock.add(link)\n",
    "\n",
    "            pre_market_price_for_stock = get_pre_market_price_for_stock(soup)\n",
    "            prev_close_price_for_stock = get_prev_close(soup)\n",
    "\n",
    "            res = {\n",
    "                \"articles_for_stock\": list(articles_for_stock),\n",
    "            }\n",
    "\n",
    "            if pre_market_price_for_stock:\n",
    "                res[\"pre_market_price\"] = pre_market_price_for_stock\n",
    "                print(\"Found pre market price \", pre_market_price_for_stock)\n",
    "            if prev_close_price_for_stock:\n",
    "                res[\"prev_close\"] = prev_close_price_for_stock\n",
    "                print(\"Found prev_close \", prev_close_price_for_stock)\n",
    "                \n",
    "            return res\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e363da-2fd8-4a7a-ad6a-6c72f85cec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories_for_stock(articles_for_stock, stock, opts, svc):\n",
    "    if not articles_for_stock:\n",
    "        return\n",
    "\n",
    "    with webdriver.Chrome(service=svc, options=opts) as driver:\n",
    "        stories_for_stock = []\n",
    "\n",
    "        # these are all the stories for the stock\n",
    "        for link in list(articles_for_stock):\n",
    "            print(f\"[scraper] scraping {link}, stock {stock}\")\n",
    "            try: \n",
    "                story = scrape_recent_news_for_sym(link, driver)\n",
    "                if not story:\n",
    "                    continue\n",
    "                stories_for_stock.append(story)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to get story for \", link)\n",
    "                continue\n",
    "        \n",
    "        return stories_for_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201b8ac-07ed-4b19-af2d-c39bff83d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scraped_stock_data(res, stock, run_id):\n",
    "    if any(key not in res for key in [\"pre_market_price\", \"prev_close\"]):\n",
    "        return\n",
    "        \n",
    "    db = get_db()\n",
    "    scrapes_collection = db['stock_prices']\n",
    "\n",
    "    doc = {\n",
    "        \"pre_market_price\": res[\"pre_market_price\"],\n",
    "        \"prev_close\": res[\"prev_close\"],\n",
    "        \"created_at\": datetime.now(timezone.utc),\n",
    "        \"stock\": stock,\n",
    "        \"run_id\": run_id,\n",
    "    }\n",
    "    scrapes_collection.insert_one(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a9676-d666-4e7d-87f2-3f8bff485bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scraper(stock, timestamp, run_id, opts, svc, worker_idx):\n",
    "    url = f\"https://finance.yahoo.com/quote/{stock}\"\n",
    "    print(f\"[scraper] getting articles for url {url}, idx {worker_idx}\")\n",
    "\n",
    "    try:\n",
    "        scraped_stock_res = get_articles_for_stock(url, opts, svc)\n",
    "        articles_for_stock = scraped_stock_res[\"articles_for_stock\"]\n",
    "        if not articles_for_stock:\n",
    "            print(f\"[scraper] no articles found for stock {stock}\")\n",
    "            return\n",
    "            \n",
    "        stories_for_stock = get_stories_for_stock(articles_for_stock, stock, opts, svc)\n",
    "        if not stories_for_stock:\n",
    "            print(f\"No stories found for stock {stock}\")\n",
    "            return\n",
    "\n",
    "        print(f\"[scraper] found {len(stories_for_stock)} for stock {stock}\")\n",
    "        print(f\"[scraper] Saving articles to storage for stock {stock}\")\n",
    "        save_articles_to_storage(stories_for_stock, stock, timestamp, run_id)\n",
    "        \n",
    "        save_scraped_stock_data(scraped_stock_res, stock, run_id)\n",
    "        print(f\"[scraper] completed for stock {stock}, ts: {timestamp}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise ValueError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d4c93-694b-400b-9364-bad7f0905f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_job(stock, timestamp, sema, run_id, worker_idx):\n",
    "    sema.acquire()       \n",
    "    opts = webdriver.ChromeOptions()\n",
    "\n",
    "    # opts.add_argument(\"--headless\")\n",
    "    # opts.add_argument(\"--disable-gpu\")\n",
    "    # opts.add_argument(\"window-size=1920,1080\")\n",
    "    # opts.add_argument(\"--no-sandbox\")\n",
    "    # opts.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "    svc = ChromeService(ChromeDriverManager().install())\n",
    "\n",
    "    try: \n",
    "        run_scraper(stock, timestamp, run_id, opts, svc, worker_idx)\n",
    "        print(f\"[scraper] SUCCESS on stock {stock}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"[scraper] FAILED on stock {stock}\")\n",
    "    finally:\n",
    "        sema.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f52fcb-c301-41fa-94c1-dda1abc6a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(run_id, stocks):\n",
    "    maxthreads = 5\n",
    "    sema = threading.Semaphore(value=maxthreads)\n",
    "    threads = list()\n",
    "\n",
    "    utc_now = datetime.now(timezone.utc)\n",
    "\n",
    "    for idx, stock in enumerate(stocks):\n",
    "        args = (stock, utc_now, sema, run_id, idx)\n",
    "        thread = threading.Thread(target=run_job, args=args)\n",
    "        threads.append(thread)\n",
    "    \n",
    "    for thread in threads:\n",
    "        time.sleep(5)\n",
    "        thread.start()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03eba3-7ee4-48f0-b220-496c0c84f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _start(run_id, stocks):\n",
    "    # Get the current time in UTC\n",
    "    current_time_utc = datetime.now(timezone.utc)\n",
    "    print(\"Current time is \", current_time_utc)\n",
    "    date_string = \"2024-10-08\"\n",
    "    thirty_min = 1800 # seconds\n",
    "    \n",
    "    \n",
    "    # Convert to a datetime object\n",
    "    date_object = datetime.strptime(date_string, \"%Y-%m-%d\").date()\n",
    "    target_time = datetime.combine(date_object, dt_time(12, 0, 0), timezone.utc)\n",
    "    # target_time = datetime.combine(date_object, dt_time(5, 33, 0), timezone.utc)\n",
    "    \n",
    "    while current_time_utc < target_time:\n",
    "        print(\"Current time is \", current_time_utc)\n",
    "        time.sleep(thirty_min)\n",
    "        current_time_utc = datetime.now(timezone.utc)\n",
    "\n",
    "    start(run_id, stocks)\n",
    "    print(\"Now it's after\", current_time_utc)\n",
    "    print(current_time_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90330d75-eb73-4d48-854a-bb1bc3050d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks = get_stocks_list()\n",
    "stocks = [\n",
    "    \"WMT\", \"AMZN\", \"AAPL\", \"CVS\", \"UNH\", \"XOM\", \"BRK.B\", \"GOOGL\", \"MCK\", \"CVX\", \n",
    "    \"ABC\", \"COST\", \"MSFT\", \"CAH\", \"CI\", \"MPC\", \"PSX\", \"VLO\", \"F\", \"HD\", \"JPM\", \n",
    "    \"GM\", \"ANTM\", \"GE\", \"KR\", \"CMCSA\", \"T\", \"VZ\", \"DELL\", \"TGT\", \"SHEL\", \"BA\", \n",
    "    \"WBA\", \"BAC\", \"WFC\", \"C\", \"JCI\", \"JNJ\", \"IBM\", \"FMCC\", \"HUM\", \"PEP\", \"UPS\", \n",
    "    \"INTC\", \"PG\", \"ADM\", \"ACI\", \"MET\", \"GS\", \"SYY\", \"RTX\", \"HPQ\", \"BA\", \"CNC\", \n",
    "    \"LOW\", \"FDX\", \"MRK\", \"CAT\", \"DIS\", \"PFE\", \"LMT\", \"MS\", \"CSCO\", \"KO\", \"ABBV\", \n",
    "    \"ALL\", \"AIG\", \"DAL\", \"CHTR\", \"NYL\", \"AXP\", \"NFS\", \"BBY\", \"LMIC\", \"MRK\", \"TSN\", \n",
    "    \"UAL\", \"TJX\", \"PGR\", \"DE\", \"ABT\", \"GD\", \"KO\", \"NKE\", \"HCA\", \"JBL\", \"AAL\", \n",
    "    \"MDLZ\", \"TIAA\", \"CI\", \"PUSH\", \"COP\", \"GIS\", \"TMO\", \"BMY\", \"GS\", \"EPD\", \"USAA\", \n",
    "    \"PM\", \"DHR\", \"NWM\", \"RAD\", \"MMM\", \"SBUX\", \"QCOM\", \"NOC\", \"COF\", \"TRV\", \"ARW\", \n",
    "    \"HON\", \"DG\", \"DOW\", \"WHR\", \"ARMK\", \"PFGC\", \"CHSCP\", \"PBF\"\n",
    "]\n",
    "\n",
    "# run_id = str(uuid.uuid4())\n",
    "run_id = \"c9eb6450-dafc-45c1-bce5-2cdfb81cc850\"\n",
    "print(f\"[scraper] Starting Yahoo scraper on {len(stocks)} stocks, run id {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4f02e-fdfb-410b-93d0-f9c0dbde94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start(run_id, stocks)\n",
    "# _start(run_id, stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7e40d-4035-471c-a4a6-603a02614ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70976a5-8fdc-47e9-ab01-2fc17bb75669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d415e9d-d02b-4944-b2f7-1a23a6d611aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
