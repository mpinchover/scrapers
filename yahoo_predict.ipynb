{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3932d155-2970-44b1-b122-b083d8182664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run predictions on individual stocks\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta, time as dt_time\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import uuid\n",
    "import traceback\n",
    "from google.cloud import storage\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./svc_acc_key.json\"\n",
    "MONGO_URI=os.environ[\"MONGO_URI\"]\n",
    "OPENAI_API_KEY=os.environ[\"OPENAI_API_KEY\"]\n",
    "STORAGE_BUCKET=os.environ[\"STORAGE_BUCKET\"]\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "MAX_LEN_WORDS_PER_REQ = 60000\n",
    "db = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "018118ad-06db-4273-a3fa-a6b38ca81b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client.from_service_account_json(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])\n",
    "bucket = storage_client.get_bucket(STORAGE_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fd7e745b-d6e7-4012-b37b-59e8b1bf60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(MONGO_URI, server_api=ServerApi('1'))\n",
    "db = client.get_database()\n",
    "\n",
    "def get_db():\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bea6b6ce-9cf3-425a-960d-cc27d9b1835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_saved_articles_from_storage(scrapes):\n",
    "    articles = []\n",
    "    for scrape in scrapes:\n",
    "        if 'bucket_key' not in scrape:\n",
    "            # print(\"Bucket key not found\")\n",
    "            continue\n",
    "\n",
    "        key = scrape['bucket_key']\n",
    "        blob = bucket.blob(key)\n",
    "        content = blob.download_as_string()\n",
    "        if not content:\n",
    "            print(\"Couldn't get article content from bucket\")\n",
    "            continue\n",
    "\n",
    "        if len(content) < 100:\n",
    "            print(\"article is too short, skipping\")\n",
    "            continue \n",
    "        \n",
    "        try:\n",
    "        # Try converting the content to a JSON object\n",
    "            json_content = json.loads(content)\n",
    "            articles.append(json_content)\n",
    "        except Exception as e:\n",
    "            # If conversion fails, just skip and continue\n",
    "            continue\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "40cac7a8-4093-42b0-ad26-a08a9d026e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_openai_resp_as_csv(df, df_filtered, run_id, lookback):   \n",
    "    cur_time = datetime.now(timezone.utc)     \n",
    "    key = f\"predictions/{run_id}/{lookback}_predictions.csv\"\n",
    "    csv_data = df.to_csv(index=False)\n",
    "\n",
    "    # Create a blob (the object in GCS)\n",
    "    blob = bucket.blob(key)\n",
    "\n",
    "    # Upload the CSV string as a file to the bucket\n",
    "    blob.upload_from_string(csv_data, content_type='text/csv')\n",
    "    db = get_db()\n",
    "    db[\"predictions\"].insert_one({\n",
    "        \"bucket_key\": key,\n",
    "        \"lookback\": lookback,\n",
    "        \"predicted_at\": cur_time,\n",
    "    })\n",
    "\n",
    "    key_filtered = f\"predictions/{run_id}/{lookback}_predictions_filtered.csv\"\n",
    "    blob_filtered = bucket.blob(key_filtered)\n",
    "    csv_data_filtered = df_filtered.to_csv(index=False)\n",
    "    blob_filtered.upload_from_string(csv_data_filtered, content_type='text/csv')\n",
    "    print(f\"saved {lookback}_predictions.csv\")\n",
    "\n",
    "def generate_analysis_for_article(stock_sym, article):\n",
    "    client = OpenAI()\n",
    "    system_content = f\"\"\"\n",
    "    For the given article, predict if {stock_sym} will rise in the next trading window.\n",
    "    If the article does not mention anything about {stock_sym}, return an answer of NA.\n",
    "    Otherwise, return a YES or NO. The only acceptable responses are YES, NO or NA.\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": article\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    resp = completion.choices[0].message.content\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ff428e0f-2f0c-45c7-82e6-488be277630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stocks_list(run_id, lookback): \n",
    "    cur_time = datetime.now(timezone.utc)\n",
    "\n",
    "    print(f\"[predict] Get stocks list with lookback {lookback}, from time {cur_time}\")\n",
    "\n",
    "    lookback_from = cur_time - timedelta(hours=lookback) # change this to 6 hours lookback\n",
    "    dup_articles_set = set()\n",
    "\n",
    "    db = get_db()\n",
    "    collection = db['scrapes']\n",
    "    stock_prices_col = db[\"stock_prices\"]\n",
    "\n",
    "    # get everything from the run id that has been published in the last {lookback} hours\n",
    "    recent_scrapes = collection.find({\"run_id\": run_id, \"published_at\": {\"$gte\": lookback_from}})\n",
    "\n",
    "    recent_scrapes_dict = {}\n",
    "    for scrape in recent_scrapes:\n",
    "        if 'url' not in scrape:\n",
    "            print(\"url is not in scrape\")\n",
    "            continue\n",
    "\n",
    "        url_key = scrape['url']\n",
    "        if url_key in dup_articles_set:\n",
    "            continue \n",
    "\n",
    "        dup_articles_set.add(url_key)\n",
    "\n",
    "        if \"stock\" not in scrape:\n",
    "            continue\n",
    "  \n",
    "        stock = scrape['stock'].lower()\n",
    "        if stock not in recent_scrapes_dict:\n",
    "            recent_scrapes_dict[stock] = {\n",
    "                \"scrapes\": []\n",
    "            }\n",
    "            del scrape[\"stock\"]\n",
    "\n",
    "        stock_data = recent_scrapes_dict[stock]\n",
    "        stock_data[\"scrapes\"].append(scrape)\n",
    "        # recent_scrapes_dict[stock].append(scrape)\n",
    "    \n",
    "    # stock_prices = stock_prices_col.find({\"run_id\": run_id})\n",
    "    # for sp in stock_prices:\n",
    "    #     if not all(key in sp for key in [\"stock\", \"pre_market_price\", \"prev_close\"]):\n",
    "    #         continue\n",
    "            \n",
    "    #     stock = sp[\"stock\"]\n",
    "    #     if stock not in recent_scrapes_dict:\n",
    "    #         continue \n",
    "            \n",
    "    #     pre_market_price = sp[\"pre_market_price\"]\n",
    "    #     prev_close = sp[\"prev_close\"] = sp[\"prev_close\"]\n",
    "        \n",
    "    #     recent_scrapes_dict[stock][\"pre_market_price\"] = pre_market_price\n",
    "    #     recent_scrapes_dict[stock][\"prev_close\"] = prev_close\n",
    "        \n",
    "    print(f\"[predict] found {len(dup_articles_set)} duplicate articles\")\n",
    "    return recent_scrapes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "280914f4-de2b-43ec-bfbf-a3ac48855784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_published_at(file_content_formatted, source):\n",
    "    published_at = None\n",
    "        \n",
    "    if \"published_at\" in file_content_formatted:\n",
    "        time_as_str = file_content_formatted[\"published_at\"]\n",
    "        if not time_as_str:\n",
    "            print(f\"time not found for {file_content_formatted[\"title\"]}\")\n",
    "            return \n",
    "\n",
    "        try:\n",
    "            published_at = datetime.strptime(time_as_str, '%Y-%m-%dT%H:%M:%S.%fZ').replace(tzinfo=timezone.utc)\n",
    "        except ValueError:\n",
    "            # If that fails, try without milliseconds\n",
    "            published_at = datetime.strptime(time_as_str, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=timezone.utc)\n",
    "    return published_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3ffb6eae-ac35-41e0-a759-c6696940b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_for_stock(stock_sym, scrapes_for_stock):\n",
    "    print(f\"Generate analysis for stock {stock_sym}\")\n",
    "    # you need to get all the articles into an array based on the scrapes\n",
    "\n",
    "    saved_articles = collect_saved_articles_from_storage(scrapes_for_stock)\n",
    "    print(f\"Got saved articles of length {len(saved_articles)} for stock {stock_sym}\")\n",
    "\n",
    "    all_responses = []\n",
    "    for article in saved_articles:\n",
    "        if \"content\" not in article:\n",
    "            print(\"skipping openai, 'content' not found in article\")\n",
    "            continue \n",
    "        article_content = article['content']\n",
    "\n",
    "        try:\n",
    "            resp = generate_analysis_for_article(stock_sym, article_content)\n",
    "            formatted_resp = resp.lower()\n",
    "            if \"yes\" in formatted_resp:\n",
    "                all_responses.append(\"YES\")\n",
    "            elif \"no\" in formatted_resp:\n",
    "                all_responses.append(\"NO\")\n",
    "            else:\n",
    "                all_responses.append(\"NA\")\n",
    "        except Exception as e:\n",
    "            print(f\"failure calling openai: {e}\")\n",
    "            \n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bbf32e2c-838d-49c9-953b-85ac2d4ef9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rows_to_csv(rows):\n",
    "    # Initialize a list to hold the rows for the DataFrame\n",
    "    data = []\n",
    "    \n",
    "    # Process each key-value pair in the dictionary\n",
    "    for symbol, values in rows.items():\n",
    "        # Count occurrences of \"YES\", \"NO\", and \"NA\"\n",
    "        yes_count = values.count('YES')\n",
    "        no_count = values.count('NO')\n",
    "        na_count = values.count('NA')\n",
    "        \n",
    "        # Append the data as a new row\n",
    "        data.append([symbol, yes_count, no_count, na_count])\n",
    "    \n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data, columns=['Symbol', 'YES', 'NO', 'NA'])\n",
    "    # Sort the DataFrame by the \"YES\" column in descending order\n",
    "    df.sort_values(by='YES', ascending=False, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ba8c29cb-462a-4675-8e06-9035fa4a3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_order_df(df):\n",
    "    # Filter rows where \"YES\" count is greater than 1 and greater than \"NO\" count\n",
    "    filtered_df = df[(df['YES'] > 1) & (df['YES'] > df['NO'])].copy()\n",
    "    \n",
    "    # Calculate the difference and add it as a new column using .loc\n",
    "    filtered_df.loc[:, 'YES_NO_DIFF'] = filtered_df['YES'] - filtered_df['NO']\n",
    "    \n",
    "    # Sort the DataFrame by the \"YES_NO_DIFF\" in descending order\n",
    "    filtered_df.sort_values(by='YES_NO_DIFF', ascending=False, inplace=True)\n",
    "    \n",
    "    # Drop the \"YES_NO_DIFF\" column as it's not needed in the final output\n",
    "    filtered_df.drop(columns=['YES_NO_DIFF'], inplace=True)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6893d649-cbee-4661-8b45-def0edaea664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(run_id, lookback):\n",
    "    stocks = get_stocks_list(run_id, lookback)\n",
    "    if not stocks:\n",
    "        print(f\"[predict] not stocks found for prediction\")\n",
    "        return \n",
    "\n",
    "    print(f\"[predict] creating prediction for {len(stocks)}\")\n",
    "    rows = {}\n",
    "    current_time = datetime.now().astimezone(timezone.utc)\n",
    "\n",
    "    for stock, stock_data in stocks.items():\n",
    "        stock_sym = stock.lower()\n",
    "        scrapes_for_stock = stock_data[\"scrapes\"]\n",
    "        \n",
    "        responses = generate_analysis_for_stock(stock_sym, scrapes_for_stock)\n",
    "        if not responses:\n",
    "            continue\n",
    "            \n",
    "        rows[stock_sym] = responses\n",
    "    df = convert_rows_to_csv(rows)\n",
    "    df_filtered = filter_and_order_df(df)\n",
    "    save_openai_resp_as_csv(df, df_filtered, run_id, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0d747227-147a-471f-b50e-59a2815f1c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id:  c9eb6450-dafc-45c1-bce5-2cdfb81cc850\n"
     ]
    }
   ],
   "source": [
    "run_id = \"c9eb6450-dafc-45c1-bce5-2cdfb81cc850\"\n",
    "lookback = 24\n",
    "print(\"run_id: \", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9f539cb2-f672-46d9-bb25-833b183f3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(run_id, lookback):\n",
    "    run_analysis(run_id, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d673ca15-67da-4dca-85d8-6a3d3a09b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _start(run_id, lookback):\n",
    "\n",
    "    current_time_utc = datetime.now(timezone.utc)\n",
    "    print(\"Current time is \", current_time_utc)\n",
    "    date_string_to_execute = \"2024-10-08\"\n",
    "\n",
    "    # Convert to a datetime object\n",
    "    date_object = datetime.strptime(date_string_to_execute, \"%Y-%m-%d\").date()\n",
    "    target_time = datetime.combine(date_object, dt_time(5, 21, 0), timezone.utc)\n",
    "    \n",
    "    while current_time_utc < target_time:\n",
    "        print(\"Waiting to launch...\", current_time_utc)\n",
    "        time.sleep(5)\n",
    "        current_time_utc = datetime.now(timezone.utc)\n",
    "\n",
    "    run_analysis(run_id, lookback)\n",
    "    print(\"Now it's after\", current_time_utc)\n",
    "    print(current_time_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "292e8d7b-84d5-4019-a37e-f6056ddbb0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] Get stocks list with lookback 24, from time 2024-10-08 23:31:51.690498+00:00\n",
      "[predict] found 87 duplicate articles\n",
      "[predict] creating prediction for 39\n",
      "Generate analysis for stock bkr\n",
      "Got saved articles of length 1 for stock bkr\n",
      "Generate analysis for stock dltr\n",
      "Got saved articles of length 1 for stock dltr\n",
      "Generate analysis for stock mo\n",
      "Got saved articles of length 1 for stock mo\n",
      "Generate analysis for stock thc\n",
      "Got saved articles of length 1 for stock thc\n",
      "Generate analysis for stock oke\n",
      "Got saved articles of length 1 for stock oke\n",
      "Generate analysis for stock dxc\n",
      "Got saved articles of length 2 for stock dxc\n",
      "Generate analysis for stock ma\n",
      "Got saved articles of length 1 for stock ma\n",
      "Generate analysis for stock khc\n",
      "Got saved articles of length 1 for stock khc\n",
      "Generate analysis for stock acm\n",
      "Got saved articles of length 3 for stock acm\n",
      "Generate analysis for stock ip\n",
      "Got saved articles of length 1 for stock ip\n",
      "Generate analysis for stock v\n",
      "Got saved articles of length 1 for stock v\n",
      "Generate analysis for stock j\n",
      "Got saved articles of length 1 for stock j\n",
      "Generate analysis for stock nvda\n",
      "Got saved articles of length 11 for stock nvda\n",
      "Generate analysis for stock cl\n",
      "Got saved articles of length 1 for stock cl\n",
      "Generate analysis for stock bdx\n",
      "Got saved articles of length 1 for stock bdx\n",
      "Generate analysis for stock jll\n",
      "Got saved articles of length 1 for stock jll\n",
      "Generate analysis for stock mar\n",
      "Got saved articles of length 2 for stock mar\n",
      "Generate analysis for stock cag\n",
      "Got saved articles of length 1 for stock cag\n",
      "Generate analysis for stock amp\n",
      "Got saved articles of length 1 for stock amp\n",
      "Generate analysis for stock ctsh\n",
      "Got saved articles of length 1 for stock ctsh\n",
      "Generate analysis for stock pfg\n",
      "Got saved articles of length 1 for stock pfg\n",
      "Generate analysis for stock amzn\n",
      "Got saved articles of length 5 for stock amzn\n",
      "Generate analysis for stock googl\n",
      "Got saved articles of length 11 for stock googl\n",
      "Generate analysis for stock aapl\n",
      "Got saved articles of length 4 for stock aapl\n",
      "Generate analysis for stock brk.b\n",
      "Got saved articles of length 9 for stock brk.b\n",
      "Generate analysis for stock cost\n",
      "Got saved articles of length 1 for stock cost\n",
      "Generate analysis for stock msft\n",
      "Got saved articles of length 2 for stock msft\n",
      "Generate analysis for stock cvx\n",
      "Got saved articles of length 4 for stock cvx\n",
      "Generate analysis for stock f\n",
      "Got saved articles of length 1 for stock f\n",
      "Generate analysis for stock jpm\n",
      "Got saved articles of length 1 for stock jpm\n",
      "Generate analysis for stock gm\n",
      "Got saved articles of length 3 for stock gm\n",
      "Generate analysis for stock kr\n",
      "Got saved articles of length 1 for stock kr\n",
      "Generate analysis for stock ge\n",
      "Got saved articles of length 1 for stock ge\n",
      "Generate analysis for stock tgt\n",
      "Got saved articles of length 1 for stock tgt\n",
      "Generate analysis for stock wba\n",
      "Got saved articles of length 1 for stock wba\n",
      "Generate analysis for stock met\n",
      "Got saved articles of length 2 for stock met\n",
      "Generate analysis for stock tsn\n",
      "Got saved articles of length 1 for stock tsn\n",
      "Generate analysis for stock mrk\n",
      "Got saved articles of length 1 for stock mrk\n",
      "Generate analysis for stock nke\n",
      "Got saved articles of length 3 for stock nke\n",
      "saved 24_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "start(run_id, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fa59c-a02a-4dbb-94df-ea47fbc42376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b5b0b-f294-48f3-9c44-ddd4f7e5d9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
